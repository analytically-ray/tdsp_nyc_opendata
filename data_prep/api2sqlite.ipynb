{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8257aecf-d565-4447-83d4-2aa5a4b6a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import time\n",
    "from urllib.parse import urlencode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b05fae-c498-4018-bb18-9563493e3551",
   "metadata": {},
   "source": [
    "After importing the necessary packages, start defining constants, including the base API URL for the raw data, the name of the database and table, and the batch size for loading data into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "576eec88-5b5a-4a88-a8a6-63a3b1ba52a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BASE_URL = \"https://data.cityofnewyork.us/resource/h9gi-nx95.csv\"\n",
    "DB_PATH = \"nyc_crashes.db\"\n",
    "TABLE_NAME = \"crashes\"\n",
    "BATCH_SIZE = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f14d7-1233-4097-9907-a156b4af3905",
   "metadata": {},
   "source": [
    "Next, define the connection to the database using the defined database name. Define more constants to initiate the while loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "841a0fca-9db3-4b86-82e0-4173bb80ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "total_fetched = 0\n",
    "batch_num = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f65b23-09ae-4719-879f-a48ed5360882",
   "metadata": {},
   "source": [
    "Let's start running the while loop. We start be defining the offset as batch_num * batch_size. Output which batch and records are being loaded for user verification and updates.\n",
    "Next, try loading the csv using the base API URL with some parameter adders including ordering the data by latest crash date, the size of the batch and the offset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a208e7-25a8-472a-a0c3-8cdfd78725c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching batch 1, records 1 to 50000 ...\n",
      "Error fetching batch at offset 0: too many SQL variables\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    offset = batch_num * BATCH_SIZE\n",
    "    print(f\"\\nFetching batch {batch_num + 1}, records {offset + 1} to {offset + BATCH_SIZE} ...\")\n",
    "\n",
    "    params = {\n",
    "        \"$order\": \"crash_date DESC\",\n",
    "        \"$limit\": BATCH_SIZE,\n",
    "        \"$offset\": offset\n",
    "    }\n",
    "    query_string = urlencode(params)\n",
    "    url = f\"{BASE_URL}?{query_string}\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            url,\n",
    "            parse_dates=[\"crash_date\"],\n",
    "            dtype={\"crash_time\": str, \"zip_code\": str, \"collision_id\": str}  # Ensure collision_id is string\n",
    "        )\n",
    "        if df.empty:\n",
    "            print(\"No more data to fetch, exiting.\")\n",
    "            break\n",
    "\n",
    "        # Normalize column names: lowercase and replace spaces with underscores\n",
    "        df.columns = [col.lower().replace(\" \", \"_\") for col in df.columns]\n",
    "\n",
    "        # Get existing collision_ids from DB to identify duplicates\n",
    "        batch_collision_ids = df[\"collision_id\"].astype(str).tolist()\n",
    "\n",
    "        if batch_collision_ids:\n",
    "            placeholders = \",\".join(\"?\" for _ in batch_collision_ids)\n",
    "            query = f\"SELECT collision_id FROM {TABLE_NAME} WHERE collision_id IN ({placeholders})\"\n",
    "            cursor.execute(query, batch_collision_ids)\n",
    "            existing_ids = set(row[0] for row in cursor.fetchall())\n",
    "        else:\n",
    "            existing_ids = set()\n",
    "\n",
    "        # Filter df to only unique rows (collision_id not in existing_ids)\n",
    "        df_unique = df[~df[\"collision_id\"].astype(str).isin(existing_ids)]\n",
    "\n",
    "        if df_unique.empty:\n",
    "            print(\"No unique records found in batch. Stopping fetch.\")\n",
    "            break\n",
    "\n",
    "        if batch_num == 0:\n",
    "            # Create table if not exists based on this batch's schema\n",
    "            columns_with_types = []\n",
    "            for col, dtype in df_unique.dtypes.items():\n",
    "                if pd.api.types.is_integer_dtype(dtype):\n",
    "                    col_type = \"INTEGER\"\n",
    "                elif pd.api.types.is_float_dtype(dtype):\n",
    "                    col_type = \"REAL\"\n",
    "                elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "                    col_type = \"TEXT\"\n",
    "                else:\n",
    "                    col_type = \"TEXT\"\n",
    "                columns_with_types.append(f\"{col} {col_type}\")\n",
    "\n",
    "            create_table_sql = f\"CREATE TABLE IF NOT EXISTS {TABLE_NAME} ({', '.join(columns_with_types)});\"\n",
    "            cursor.execute(create_table_sql)\n",
    "            conn.commit()\n",
    "\n",
    "            df_unique.to_sql(TABLE_NAME, conn, if_exists=\"append\", index=False)\n",
    "            print(f\"Inserted first {len(df_unique)} unique records.\")\n",
    "        else:\n",
    "            df_unique.to_sql(TABLE_NAME, conn, if_exists=\"append\", index=False)\n",
    "            print(f\"Inserted {len(df_unique)} unique records.\")\n",
    "\n",
    "        total_fetched += len(df_unique)\n",
    "        batch_num += 1\n",
    "\n",
    "        time.sleep(1)  # Be polite to the API\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"No more data to fetch, exiting.\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching batch at offset {offset}: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c82d61f9-e0e0-4ee0-b684-e645c85cb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished. Total records inserted: 200000\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nFinished. Total records inserted: {total_fetched}\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffdd045-4980-4b19-bfdd-678b1ba7b4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
